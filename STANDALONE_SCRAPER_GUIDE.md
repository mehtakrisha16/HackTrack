# ğŸ¤– Standalone Opportunity Scraper Service

## ğŸš€ Overview

The Standalone Opportunity Scraper is a **24/7 autonomous data collection service** that continuously scrapes the latest 100 opportunities from 15+ major companies and platforms. It operates **independently from your main website**, ensuring real-time data availability even when your website is offline.

## âœ¨ Key Features

### ğŸ”„ **Continuous Operation**
- **Runs independently** from your main website server
- **24/7 automated scraping** every 2 hours
- **Survives server restarts** - keeps collecting data
- **Background service** - doesn't interfere with your website

### ğŸ“Š **Comprehensive Data Collection**
- **100 opportunities per source** (not just 10-20)
- **15+ major companies** including Razorpay, Paytm, Google, Microsoft
- **4 opportunity types**: Jobs, Internships, Hackathons, Events
- **Smart deduplication** to avoid duplicate entries
- **Intelligent data extraction** with fallback parsers

### ğŸ¢ **Supported Sources**

#### **FinTech Companies** (Priority: High)
- ğŸ¦ **Razorpay** - Payment gateway leader
- ğŸ’³ **Paytm** - Digital payments giant  
- ğŸ“± **PhonePe** - UPI payments platform
- ğŸ“ˆ **Zerodha** - Stock trading platform
- ğŸ¥ **Policybazaar** - Insurance marketplace

#### **Tech Giants** (Priority: Maximum)
- ğŸŒ **Google** - Search & cloud services
- ğŸ’» **Microsoft** - Software & cloud platform

#### **Hackathon Platforms** (Priority: High)
- ğŸ† **Devpost** - Global hackathon platform
- ğŸ¯ **HackerEarth** - Coding competitions
- ğŸš€ **Unstop** - Student competitions

#### **Internship Platforms** (Priority: High)
- ğŸ“š **Internshala** - India's largest internship platform
- ğŸ“ **LetsIntern** - Premium internships

#### **Job Portals** (Priority: High)
- ğŸ’¼ **LinkedIn** - Professional network
- ğŸ” **Naukri.com** - Leading job portal

## ğŸ›  Installation & Setup

### **Prerequisites**
```bash
# Ensure you have Node.js 16+ installed
node --version  # Should be 16.0.0 or higher
```

### **Quick Start**
```powershell
# Method 1: Using PowerShell Service Manager (Recommended)
.\scraper-service.ps1 -Start

# Method 2: Using Batch Script
.\start-scraper.bat

# Method 3: Direct Node.js
cd FYP_DATA
npm run scrape:standalone
```

### **Service Management**
```powershell
# Start the service
.\scraper-service.ps1 -Start

# Check status and view statistics  
.\scraper-service.ps1 -Status

# Stop the service
.\scraper-service.ps1 -Stop

# Restart the service
.\scraper-service.ps1 -Restart
```

## ğŸ“ˆ Monitoring & Analytics

### **Real-time Dashboard**
```bash
cd FYP_DATA
node monitor.js                # One-time status
node monitor.js --watch        # Auto-refresh every 30s
node monitor.js --stats        # Detailed statistics
```

### **Dashboard Shows:**
- ğŸ“Š **Total opportunities** in database
- ğŸ†• **Recently added** opportunities (24h, 7d)
- ğŸ¢ **Top companies** by opportunity count
- ğŸ“‹ **Opportunity type** distribution  
- ğŸ”§ **Scraper status** for each source
- âš¡ **Recent additions** with timestamps

### **Log Files**
```
FYP_DATA/logs/
â”œâ”€â”€ scraper.log          # Detailed scraping logs
â””â”€â”€ scraper-service.log  # Service management logs
```

## ğŸ—„ï¸ Database Schema

### **Enhanced Opportunity Model**
```javascript
{
  title: "Senior Software Engineer",           // Job title
  company: "Razorpay",                        // Company name
  location: "Bangalore",                      // Job location
  type: "job",                                // job|internship|hackathon|event
  category: "software",                       // Technology category
  description: "Build scalable payment...",   // Job description
  requirements: ["3+ years experience"],      // Job requirements array
  skills: ["JavaScript", "React", "Node.js"], // Required skills array
  salary: "â‚¹15-25 LPA",                      // Salary information
  experience: "3+ years",                     // Experience required
  deadline: "2025-01-15",                    // Application deadline
  applicationLink: "https://...",             // Direct apply link
  sourceUrl: "https://razorpay.com/jobs/",   // Source page
  benefits: ["Health Insurance", "Stock"],    // Benefits array
  remote: true,                              // Remote work option
  urgent: false,                             // Urgent hiring flag
  priority: 9,                               // Scraper priority (1-10)
  sourceId: "unique-identifier",             // Unique source ID
  scrapedAt: "2025-01-15T10:30:00Z",        // When scraped
  lastUpdated: "2025-01-15T10:30:00Z",      // Last updated
  isActive: true,                            // Active status
  viewCount: 0,                              // View tracking
  applicationCount: 0                        // Application tracking
}
```

### **Scraper Status Tracking**
```javascript
{
  source: "razorpay",                        // Source identifier
  lastRun: "2025-01-15T10:00:00Z",          // Last scrape attempt
  lastSuccess: "2025-01-15T10:00:00Z",      // Last successful scrape
  status: "success",                         // running|success|error|idle
  totalScraped: 1250,                       // Total opportunities scraped
  errorCount: 2,                            // Error count
  lastError: "Timeout error",               // Last error message
  averageRunTime: 15000,                    // Average run time (ms)
  nextRun: "2025-01-15T12:00:00Z"          // Next scheduled run
}
```

## âš™ï¸ Configuration

### **Scraping Schedule**
- **Main Scraping**: Every 2 hours (more frequent than before)
- **Data Cleanup**: Twice daily (6 AM & 6 PM)
- **Status Reports**: Every hour
- **Old Data Removal**: 45+ day old opportunities

### **Performance Settings**
```javascript
// Per-source limits
maxResults: 100,           // Get last 100 from each source (not 20)
batchSize: 3,             // Process 3 sources simultaneously  
requestDelay: 5000,       // 5s delay between batches
pageTimeout: 60000,       // 60s page load timeout
retryAttempts: 3,         // 3 retry attempts per source
```

### **Database Configuration**
```javascript
// Connection settings
serverSelectionTimeoutMS: 5000,    // 5s connection timeout
heartbeatFrequencyMS: 2000,        // 2s heartbeat
retryWrites: true,                 // Retry failed writes
retryReads: true,                  // Retry failed reads
```

## ğŸ”— API Integration

The scraped data is automatically available through your existing API endpoints:

### **API Endpoints**
```javascript
GET /api/opportunities                    // Get all opportunities
GET /api/opportunities?company=Razorpay   // Filter by company
GET /api/opportunities?type=internship    // Filter by type  
GET /api/opportunities?category=fintech   // Filter by category
GET /api/opportunities/stats             // Get statistics
GET /api/opportunities/trending          // Get trending opportunities
```

### **Frontend Integration**
Your React components automatically receive the scraped data:
```javascript
// Your existing hooks work with real data
const { opportunities, loading } = useOpportunities();
const { stats } = useOpportunityStats();
const trending = useTrendingOpportunities();
```

## ğŸ›¡ï¸ Error Handling & Recovery

### **Robust Error Management**
- âœ… **Automatic retry** on network failures
- âœ… **Graceful degradation** when sources are down
- âœ… **Connection recovery** for database issues
- âœ… **Browser crash recovery** with automatic restart
- âœ… **Memory leak prevention** with periodic cleanup

### **Monitoring Alerts**
```javascript
// Error conditions automatically logged
- Source unreachable (network issues)
- Parsing failures (website structure changes)  
- Database connection issues
- Browser automation failures
- Memory usage spikes
```

### **Data Quality Assurance**
- ğŸ” **Duplicate detection** across sources
- ğŸ“ **Data validation** before saving
- ğŸ§¹ **Automatic cleanup** of malformed data
- ğŸ“Š **Quality metrics** tracking

## ğŸš€ Production Deployment

### **Windows Service Setup**
```powershell
# Install as Windows Service (optional)
npm install -g node-windows
node install-service.js

# Or run as background process
.\scraper-service.ps1 -Start
```

### **Linux/macOS Setup**
```bash
# Using PM2 (Process Manager)
npm install -g pm2
pm2 start src/services/standaloneScraper.js --name "opportunity-scraper"
pm2 startup
pm2 save
```

### **Docker Deployment**
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
CMD ["node", "src/services/standaloneScraper.js"]
```

## ğŸ“Š Performance Metrics

### **Expected Performance**
- **Scraping Speed**: ~100 opportunities per source in 30-60 seconds
- **Total Sources**: 15+ companies/platforms  
- **Memory Usage**: ~200-500MB during scraping
- **Database Storage**: ~1-2GB for 50,000 opportunities
- **API Response Time**: <100ms for filtered queries

### **Scalability**
- **Handles 10,000+** opportunities without performance issues
- **Concurrent scraping** of multiple sources
- **Intelligent rate limiting** to respect source servers
- **Automatic load balancing** across scraping sessions

## ğŸ”§ Troubleshooting

### **Common Issues**

**Issue**: Scraper not starting
```powershell
# Solution: Check dependencies and database
npm install
.\scraper-service.ps1 -Status
```

**Issue**: No new data being collected
```javascript
// Check scraper status
node monitor.js --stats

// Restart the service
.\scraper-service.ps1 -Restart
```

**Issue**: Database connection errors
```javascript
// Test database connection
node -e "
const mongoose = require('mongoose');
mongoose.connect('mongodb://localhost:27017/hacktrack')
  .then(() => console.log('DB OK'))
  .catch(err => console.log('DB Error:', err));
"
```

**Issue**: Browser automation failures
```javascript
// Clear browser cache and restart
.\scraper-service.ps1 -Stop
# Wait 10 seconds
.\scraper-service.ps1 -Start
```

### **Debug Mode**
```bash
# Enable verbose logging
DEBUG=puppeteer* node src/services/standaloneScraper.js

# Check individual source
node -e "const scraper = require('./src/services/standaloneScraper.js'); scraper.testSingleSource('razorpay');"
```

## ğŸ¯ Benefits for Your Platform

### **For Users**
- ğŸ†• **Always fresh content** - 100 new opportunities every 2 hours
- ğŸ” **Comprehensive coverage** - 15+ major sources
- âš¡ **Fast loading** - data pre-scraped and ready
- ğŸ¯ **Relevant opportunities** - smart categorization
- ğŸ“ˆ **Trending insights** - popular opportunities highlighted

### **For Business**
- ğŸ’° **Reduced infrastructure costs** - efficient scraping
- ğŸ“Š **Rich analytics** - detailed usage patterns  
- ğŸ”’ **Reliable data source** - multiple backup sources
- ğŸš€ **Competitive advantage** - exclusive opportunity aggregation
- ğŸ“ˆ **User engagement** - fresh content drives return visits

## ğŸ”® Future Enhancements

### **Planned Features**
- ğŸ¤– **AI-powered categorization** for better organization
- ğŸ“± **Mobile app notifications** for new opportunities
- ğŸ”” **Email alerts** for matching opportunities
- ğŸ“Š **Advanced analytics dashboard** with insights
- ğŸŒ **Geographic filtering** for location-based opportunities
- ğŸ’¼ **Company culture insights** from scraped data
- ğŸ¯ **Personalized recommendations** based on user profile

---

## ğŸ“ Support

If you encounter any issues:

1. **Check the monitor**: `node monitor.js`
2. **View logs**: Check `FYP_DATA/logs/scraper.log`  
3. **Restart service**: `.\scraper-service.ps1 -Restart`
4. **Test database**: `.\scraper-service.ps1 -Status`

**Happy Scraping! ğŸš€** Your platform now has access to real-time opportunities from 15+ major sources, updated every 2 hours automatically.